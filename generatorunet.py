# -*- coding: utf-8 -*-
"""GeneratorUnet.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13HDX65DVSsNULPIhvV74XHPAHr1phbgQ
"""

""" Parts of the U-Net model """

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np

class Sigmoid_linear(nn.Module):
    def __init__(self, shape):
        super(Sigmoid_linear, self).__init__()
        if len(shape) == 4:
            out_features = shape[1] * shape[2] * shape[3]
            self.shape = shape[1:]
            self.linear = nn.Linear(1, out_features)
        if len(shape) == 1:
            out_features = shape[0]
            self.shape = shape
            self.linear = nn.Linear(1, out_features)
        self.sigmoid = nn.Sigmoid()
        
    def forward(self, w, gender):
        l = self.linear(gender)
        l = self.sigmoid(l)
        l = l.reshape(self.shape)
        return l*w

class Update_conv2d(nn.Module):

    def __init__(self, in_c, out_c, kernel_size, padding):
        super().__init__()
        self.conv = nn.Conv2d(in_c, out_c, kernel_size=kernel_size, padding=padding)
        self.wshape = [out_c, in_c, kernel_size, kernel_size]
        self.bshape = [out_c]
        self.wlinear = Sigmoid_linear(self.wshape)
        self.blinear = Sigmoid_linear(self.bshape)

    def forward(self, x, gender):
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        if gender == 'm':
            gender = torch.ones((1)).to(device)
        elif gender == 'f':
            gender = -1 * torch.ones((1)).to(device)
        new_weight = self.wlinear(self.conv.weight, gender)
        new_b = self.blinear(self.conv.bias, gender)
        output = F.conv2d(x, new_weight, new_b, padding=1)
        return output




class DoubleConv(nn.Module):
    """(convolution => [BN] => ReLU) * 2"""
    """for upsamples where input was concatnated with label"""
    def __init__(self, in_channels, out_channels, mode='down'):
        super().__init__()
        if mode == 'down':
            self.double_conv = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),
                nn.BatchNorm2d(out_channels),
                nn.ReLU(inplace=True),
                nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),
                nn.BatchNorm2d(out_channels),
                nn.ReLU(inplace=True)
            )
        if mode == 'up':
            self.conv1 = Update_conv2d(in_channels, out_channels, kernel_size=3, padding=1)
            self.bn1 = nn.BatchNorm2d(out_channels)
            self.relu1 = nn.ReLU(inplace=True)
            self.conv2 = Update_conv2d(out_channels, out_channels, kernel_size=3, padding=1)
            self.bn2 = nn.BatchNorm2d(out_channels)
            self.relu2 = nn.ReLU(inplace=True)
    def forward(self, x, mode='down', gender=None):
        if mode == 'down':
            return self.double_conv(x)
        if mode == 'up':
            out = self.conv1(x, gender)
            out = self.bn1(out)
            out = self.relu1(out)
            out = self.conv2(out, gender)
            out = self.bn2(out)
            out = self.relu2(out)
            return out


class Down(nn.Module):
    """Downscaling with maxpool then double conv"""

    def __init__(self, in_channels, out_channels):
        super().__init__()
        self.avgpool_conv = nn.Sequential(
            nn.AvgPool2d(2),
            DoubleConv(in_channels, out_channels)
        )

    def forward(self, x):
        return self.avgpool_conv(x)


class Up(nn.Module):
    """Upscaling then double conv"""

    def __init__(self, in_channels, out_channels, bilinear=True):
        super().__init__()
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        # if bilinear, use the normal convolutions to reduce the number of channels
        if bilinear:
            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)
        else:
            self.up = nn.ConvTranspose2d(in_channels // 2, in_channels // 2, kernel_size=2, stride=2)

        self.conv = DoubleConv(in_channels, out_channels, mode='up')

    def forward(self, x1, x2, label):
        x1 = self.up(x1)
        # input is CHW
        diffY = x2.size()[2] - x1.size()[2]
        diffX = x2.size()[3] - x1.size()[3]

        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,
                        diffY // 2, diffY - diffY // 2])
        
        # if type(label) == int:
        #     if label == 0:
        #         labels = torch.zeros((x1.shape[0], 2*x1.shape[1], x1.shape[2], x1.shape[3])).to(self.device)
        #     else:
        #         labels = torch.ones((x1.shape[0], 2*x1.shape[1], x1.shape[2], x1.shape[3])).to(self.device)
        # else:
        #     labels = []
        #     labels = [torch.ones((2*x1.shape[1], x1.shape[2], x1.shape[3]))
        #               if i == 1 else torch.zeros((2*x1.shape[1], x1.shape[2], x1.shape[3])) for i in label]
        #     labels = torch.stack(labels).to(self.device)
        # if you have padding issues, see
        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a
        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd
        x = torch.cat([x2, x1], dim=1)
        
        return self.conv(x, mode='up', gender=label)


class OutConv(nn.Module):
    def __init__(self, in_channels, out_channels):
        super(OutConv, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)

    def forward(self, x):
        return self.conv(x)

""" Full assembly of the parts to form the complete network """

import torch.nn.functional as F


class UNet(nn.Module):
    def __init__(self, n_channels, n_classes, bilinear=True):
        super(UNet, self).__init__()
        self.n_channels = n_channels
        self.n_classes = n_classes
        self.bilinear = bilinear

        self.inc = DoubleConv(n_channels, 64)
        self.down1 = Down(64, 128)
        self.down2 = Down(128, 256)
        self.down3 = Down(256, 512)
        self.down4 = Down(512, 512)
        self.up1 = Up(1024, 256, bilinear)
        self.up2 = Up(512, 128, bilinear)
        self.up3 = Up(256, 64, bilinear)
        self.up4 = Up(128, 64, bilinear)
        self.outc = OutConv(64, n_classes)
        print('initilized!')

    def forward(self, x, label):
        x1 = self.inc(x)
        x2 = self.down1(x1)
        x3 = self.down2(x2)
        x4 = self.down3(x3)
        x5 = self.down4(x4)
        x = self.up1(x5, x4, label)
        # print('x6 shape = ', x.shape)
        x = self.up2(x, x3, label)
        # print('x7 shape = ', x.shape)
        x = self.up3(x, x2, label)
        # print('x8 shape = ', x.shape)
        x = self.up4(x, x1, label)
        # print('x9 shape = ', x.shape)
        logits = torch.tanh(self.outc(x))
        # print('out shape = ', logits.shape)
        return logits

# from google.colab import drive
# drive.mount('/content/drive')
# % cd '/content/drive/My Drive/785_proj/'

# import torch
# from torchvision import datasets
# import torchvision.transforms as transforms

# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# # 1
# num_workers = 0
# # 2
# batch_size = 32
# # 3
# transform = transforms.Compose(
#     [transforms.Resize((64,64)),
#      transforms.ToTensor(),
#      ])
# # 4
# train_data = datasets.ImageFolder(root = 'crop_part1', transform=transform)
# train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=num_workers)

def train(model, train_loader):
    model.train()

    for epoch in range(epochs):
        avg_loss = 0.0
        # print('Epoch: {}\tAcc: {:.4f}'.format(epoch+1, evaluate(model, train_loader)))
        for batch_num, (feats, labels) in enumerate(train_loader):
            feats, labels = feats.to(device), labels.to(device)

            optimizer.zero_grad()
            outputs = model(feats, 'm')
            loss = criterion(outputs, feats)
            loss.backward()
            optimizer.step()
            
            avg_loss += loss.item()

            if batch_num % 100 == 99:
                print('Epoch: {}\tBatch: {}\tAvg-Loss: {:.4f}'.format(epoch+1, batch_num+1, avg_loss/100))
                avg_loss = 0.0    
            
            torch.cuda.empty_cache()
            del feats
            del labels
            del loss

# lr = 0.0009
# epochs = 20

# model = UNet(3, 3)
# model.to(device)
# # model = torch.load('GD.pt').to(device)
# criterion = nn.MSELoss()
# optimizer = torch.optim.Adam(model.parameters(), lr = lr)

# train(model, train_loader)